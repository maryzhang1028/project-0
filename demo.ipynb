{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryzhang1028/project-0/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project 0 - Automated Dance Formation Translator\n",
        "========================================================\n",
        "A computer vision system for tracking dancer positions in videos and generating\n",
        "formation visualizations.\n",
        "Claude and OpenAI are used to structure, generate, and debug code in this project.\n",
        "\n",
        "\n",
        "Author: Mary Zhang\n",
        "Date: 2025"
      ],
      "metadata": {
        "id": "YxF37KjJyL9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "1. Download any of the mp4 files from the sample_videos_upload folder onto your local computer\n",
        "2. Runall code\n",
        "3. Upload the sample file you desire or any dance choreography video you want (under 30 sec) when prompted in the GUI\n",
        "4. Observe gallery or download batch files"
      ],
      "metadata": {
        "id": "nQpixiYwCB6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mounting Drive - Sample Videos (Please feel free to select videos to upload from this folder)\n",
        "# Mount Drive\n",
        "\n",
        "\"\"\"\n",
        "Cell: Download Sample Videos from Google Drive\n",
        "This ensures all users get the same sample files\n",
        "\"\"\"\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Create sample_videos directory\n",
        "os.makedirs('sample_videos_upload', exist_ok=True)\n",
        "\n",
        "# Your Google Drive file IDs (from shareable links)\n",
        "sample_videos = {\n",
        "    'dance_performance_1.mp4': '10lEHcVoCWwSeXuskXELWyMn2PCY8q2d8',\n",
        "    'dance_performance_2.mp4': '1KYm3EJWdUnAhlqd6MicRqnIesU2yPuaq',\n",
        "    'dance_performance_3.mp4': '1MBOe_nNmeOHcTye2G7r4eYZZERG4TBG',\n",
        "}\n",
        "\n",
        "print(\"📥 Downloading sample videos...\")\n",
        "for filename, file_id in sample_videos.items():\n",
        "    url = f'https://drive.google.com/drive/u/2/folders/1VmhSsLlQd5B1lKican9GwahSB-YR6Mqi={file_id}'\n",
        "    output_path = f'sample_videos_upload/{filename}'\n",
        "\n",
        "    if not os.path.exists(output_path):\n",
        "        gdown.download(url, output_path, quiet=False)\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "    else:\n",
        "        print(f\"✓ {filename} already exists\")\n",
        "\n",
        "print(\"\\n✅ All sample videos ready in 'sample_videos_upload' folder!\")\n",
        "print(\"Users can also upload their own videos using the interface.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dlX_inlt-c-8",
        "outputId": "bc297c42-487c-412a-a624-3d5a4fa99f1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading sample videos...\n",
            "✓ dance_performance_1.mp4 already exists\n",
            "✓ dance_performance_2.mp4 already exists\n",
            "✓ dance_performance_3.mp4 already exists\n",
            "\n",
            "✅ All sample videos ready in 'sample_videos_upload' folder!\n",
            "Users can also upload their own videos using the interface.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Packages\n",
        "\n",
        "\"\"\"\n",
        "Run this cell first to install required packages.\n",
        "\"\"\"\n",
        "\n",
        "!pip install opencv-python ultralytics numpy matplotlib moviepy gradio scipy --quiet\n",
        "print(\"✅ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9npUTWJvcU0",
        "outputId": "901cce27-28d0-471f-da6d-8c2011a9d269",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "# Import all necessary Python libraries and modules.\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import zipfile\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")  # Use non-interactive backend for server environments\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from ultralytics import YOLO\n",
        "from moviepy.editor import VideoFileClip\n",
        "import gradio as gr\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# Suppress numpy warnings for cleaner output\n",
        "np.seterr(all=\"ignore\")\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9QCrLK8vmVo",
        "outputId": "87bb8462-60b3-4c63-bef1-c5a73458afd3",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions (Data Processing and Geometric Calculations)\n",
        "# Helper Functions\n",
        "\n",
        "\"\"\"\n",
        "Helper functions for data processing and geometric calculations.\n",
        "\"\"\"\n",
        "\n",
        "def _float(v):\n",
        "    \"\"\"\n",
        "    Safely convert any value to float.\n",
        "\n",
        "    Args:\n",
        "        v: Value to convert (can be tensor, numpy array, or scalar)\n",
        "\n",
        "    Returns:\n",
        "        float: Converted value\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return float(v.item()) if hasattr(v, \"item\") else float(v)\n",
        "    except Exception:\n",
        "        return float(v)\n",
        "\n",
        "\n",
        "def _ensure_dir(path):\n",
        "    \"\"\"\n",
        "    Create directory if it doesn't exist.\n",
        "\n",
        "    Args:\n",
        "        path (str): Directory path to create\n",
        "\n",
        "    Returns:\n",
        "        str: The same path (for chaining)\n",
        "    \"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def iou_xyxy(box_a, box_b):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) for two bounding boxes.\n",
        "\n",
        "    IoU measures the overlap between two boxes, ranging from 0 (no overlap)\n",
        "    to 1 (perfect overlap). Used for duplicate detection removal.\n",
        "\n",
        "    Args:\n",
        "        box_a (list): First box [x1, y1, x2, y2]\n",
        "        box_b (list): Second box [x1, y1, x2, y2]\n",
        "\n",
        "    Returns:\n",
        "        float: IoU score between 0 and 1\n",
        "    \"\"\"\n",
        "    x1 = max(box_a[0], box_b[0])\n",
        "    y1 = max(box_a[1], box_b[1])\n",
        "    x2 = min(box_a[2], box_b[2])\n",
        "    y2 = min(box_a[3], box_b[3])\n",
        "\n",
        "    if x2 <= x1 or y2 <= y1:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = (x2 - x1) * (y2 - y1)\n",
        "    area_a = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n",
        "    area_b = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n",
        "    union = area_a + area_b - intersection\n",
        "\n",
        "    return intersection / max(1e-6, union)\n",
        "\n",
        "\n",
        "def nms_merge(candidates, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Non-Maximum Suppression to remove duplicate detections.\n",
        "\n",
        "    When multiple detections overlap significantly, keep only the one\n",
        "    with highest confidence score. This prevents counting the same\n",
        "    dancer multiple times.\n",
        "\n",
        "    Args:\n",
        "        candidates (list): List of detection dictionaries with 'bbox' and 'conf' keys\n",
        "        iou_threshold (float): Minimum IoU to consider boxes as duplicates\n",
        "\n",
        "    Returns:\n",
        "        list: Filtered list with duplicates removed\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    keep = []\n",
        "    used = [False] * len(candidates)\n",
        "\n",
        "    # Sort by confidence (highest first)\n",
        "    order = sorted(\n",
        "        range(len(candidates)),\n",
        "        key=lambda i: candidates[i][\"conf\"],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    for i in order:\n",
        "        if used[i]:\n",
        "            continue\n",
        "\n",
        "        keep.append(candidates[i])\n",
        "        used[i] = True\n",
        "\n",
        "        # Mark overlapping boxes as used\n",
        "        for j in order:\n",
        "            if used[j]:\n",
        "                continue\n",
        "            if iou_xyxy(candidates[i][\"bbox\"], candidates[j][\"bbox\"]) >= iou_threshold:\n",
        "                used[j] = True\n",
        "\n",
        "    return keep\n",
        "\n",
        "\n",
        "def enhance_frame(rgb_image):\n",
        "    \"\"\"\n",
        "    Enhance dark regions in video frame using CLAHE.\n",
        "\n",
        "    This technique improves visibility of dancers in poorly lit areas\n",
        "    by adaptively enhancing local contrast while preventing over-amplification.\n",
        "\n",
        "    Args:\n",
        "        rgb_image (np.ndarray): Input image in RGB format\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Enhanced image with improved visibility\n",
        "    \"\"\"\n",
        "    # Convert to YCrCb color space (Y = luminance, Cr/Cb = chrominance)\n",
        "    ycrcb = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2YCrCb)\n",
        "    y, cr, cb = cv2.split(ycrcb)\n",
        "\n",
        "    # Apply CLAHE to luminance channel\n",
        "    clahe = cv2.createCLAHE(\n",
        "        clipLimit=2.0,      # Contrast limit to prevent noise amplification\n",
        "        tileGridSize=(8, 8)  # Size of grid for local histogram equalization\n",
        "    )\n",
        "    y = clahe.apply(y)\n",
        "\n",
        "    # Merge channels back\n",
        "    ycrcb = cv2.merge((y, cr, cb))\n",
        "    enhanced = cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2RGB)\n",
        "\n",
        "    # Apply gamma correction (brighten dark areas more than bright areas)\n",
        "    gamma = 1.2\n",
        "    enhanced = np.clip(\n",
        "        ((enhanced / 255.0) ** (1 / gamma)) * 255.0,\n",
        "        0, 255\n",
        "    ).astype(np.uint8)\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "\n",
        "def umeyama(src, dst, estimate_scale=True):\n",
        "    \"\"\"\n",
        "    Estimate optimal similarity transformation between two point sets.\n",
        "\n",
        "    The Umeyama algorithm finds the best rotation, translation, and scale\n",
        "    that aligns source points to destination points. Used for tracking\n",
        "    formation changes between frames.\n",
        "\n",
        "    Args:\n",
        "        src (np.ndarray): Source points, shape (N, 2)\n",
        "        dst (np.ndarray): Destination points, shape (N, 2)\n",
        "        estimate_scale (bool): Whether to estimate scale factor\n",
        "\n",
        "    Returns:\n",
        "        tuple: (scale, rotation_matrix, translation_vector)\n",
        "    \"\"\"\n",
        "    src = np.asarray(src, dtype=np.float64)\n",
        "    dst = np.asarray(dst, dtype=np.float64)\n",
        "\n",
        "    assert src.shape == dst.shape and src.shape[1] == 2\n",
        "\n",
        "    n = src.shape[0]\n",
        "\n",
        "    # Center the point sets\n",
        "    mu_src = src.mean(0)\n",
        "    mu_dst = dst.mean(0)\n",
        "    src_centered = src - mu_src\n",
        "    dst_centered = dst - mu_dst\n",
        "\n",
        "    # Compute covariance matrix\n",
        "    covariance = (dst_centered.T @ src_centered) / n\n",
        "\n",
        "    # Singular Value Decomposition\n",
        "    U, D, Vt = np.linalg.svd(covariance)\n",
        "\n",
        "    # Ensure proper rotation (det(R) = 1, not -1)\n",
        "    S = np.eye(2)\n",
        "    if np.linalg.det(U @ Vt) < 0:\n",
        "        S[1, 1] = -1.0\n",
        "\n",
        "    # Compute rotation matrix\n",
        "    R = U @ S @ Vt\n",
        "\n",
        "    # Compute scale if requested\n",
        "    if estimate_scale:\n",
        "        var_src = (src_centered ** 2).sum() / n\n",
        "        scale = np.trace(np.diag(D) @ S) / (var_src + 1e-12)\n",
        "    else:\n",
        "        scale = 1.0\n",
        "\n",
        "    # Compute translation\n",
        "    t = mu_dst - scale * (R @ mu_src)\n",
        "\n",
        "    return float(scale), R, t"
      ],
      "metadata": {
        "id": "6Vp9p1Wiv1Ed",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dancer Formation Rendering Class\n",
        "# Formation REndering Class\n",
        "\n",
        "\"\"\"\n",
        "Class for visualizing dancer positions on a stage diagram.\n",
        "\"\"\"\n",
        "\n",
        "class FormationRenderer:\n",
        "    \"\"\"\n",
        "    Renders dancer formations on a virtual stage with grid lines and position markers.\n",
        "\n",
        "    The renderer creates a top-down view of the stage showing:\n",
        "    - Grid lines for spatial reference\n",
        "    - Colored circles for each dancer with ID numbers\n",
        "    - Stage boundaries and orientation labels\n",
        "    - Center stage marker\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stage_width=16, stage_height=10):\n",
        "        \"\"\"\n",
        "        Initialize the formation renderer.\n",
        "\n",
        "        Args:\n",
        "            stage_width (float): Stage width in grid units (default 16)\n",
        "            stage_height (float): Stage depth in grid units (default 10)\n",
        "        \"\"\"\n",
        "        self.stage_width = float(stage_width)\n",
        "        self.stage_height = float(stage_height)\n",
        "\n",
        "        # Extended color palette for up to 24 dancers\n",
        "        self.colors = [\n",
        "            \"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#96CEB4\", \"#FECA57\", \"#DDA0DD\",\n",
        "            \"#98D8C8\", \"#F7DC6F\", \"#BB8FCE\", \"#85C1E2\", \"#F8B500\", \"#FF6F61\",\n",
        "            \"#00B894\", \"#6C5CE7\", \"#E84393\", \"#00CEC9\", \"#FDCB6E\", \"#74B9FF\",\n",
        "            \"#55EFC4\", \"#A29BFE\", \"#E17055\", \"#E84393\", \"#E1BEE7\", \"#26A69A\"\n",
        "        ]\n",
        "\n",
        "        # Visual parameters\n",
        "        self.dancer_radius = 0.35    # Size of dancer circles\n",
        "        self.line_width = 2.5        # Border width for circles\n",
        "        self.opacity = 0.95          # Transparency of dancer markers\n",
        "\n",
        "    def render(self, stage_points, target_width, target_height):\n",
        "        \"\"\"\n",
        "        Render formation visualization as an image.\n",
        "\n",
        "        Args:\n",
        "            stage_points (list): List of dicts with 'id', 'x', 'y' keys for each dancer\n",
        "            target_width (int): Output image width in pixels\n",
        "            target_height (int): Output image height in pixels\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: RGB image array of the rendered formation\n",
        "        \"\"\"\n",
        "        # Setup matplotlib figure with dark background\n",
        "        dpi = 100\n",
        "        fig = plt.figure(\n",
        "            figsize=(target_width / dpi, target_height / dpi),\n",
        "            dpi=dpi,\n",
        "            facecolor=\"#2C2C2C\"  # Dark gray background\n",
        "        )\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.set_facecolor(\"#2C2C2C\")\n",
        "\n",
        "        # Draw grid lines for spatial reference\n",
        "        for x in range(int(self.stage_width) + 1):\n",
        "            ax.axvline(x, color=\"#444444\", linewidth=0.5, alpha=0.3)\n",
        "        for y in range(int(self.stage_height) + 1):\n",
        "            ax.axhline(y, color=\"#444444\", linewidth=0.5, alpha=0.3)\n",
        "\n",
        "        # Draw stage boundary (red outline)\n",
        "        stage_x = [0, self.stage_width, self.stage_width, 0, 0]\n",
        "        stage_y = [0, 0, self.stage_height, self.stage_height, 0]\n",
        "        ax.plot(stage_x, stage_y, color=\"#FF1744\", linewidth=2)\n",
        "\n",
        "        # Draw each dancer\n",
        "        for dancer in stage_points:\n",
        "            dancer_id = int(dancer[\"id\"])\n",
        "\n",
        "            # Constrain position to stage boundaries\n",
        "            x = max(0.0, min(self.stage_width, float(dancer[\"x\"])))\n",
        "            y = max(0.0, min(self.stage_height, float(dancer[\"y\"])))\n",
        "\n",
        "            # Select color from palette (cycles if more dancers than colors)\n",
        "            color = self.colors[(dancer_id - 1) % len(self.colors)]\n",
        "\n",
        "            # Create circle for dancer\n",
        "            circle = Circle(\n",
        "                (x, y),\n",
        "                self.dancer_radius,\n",
        "                facecolor=color,\n",
        "                edgecolor=\"white\",\n",
        "                linewidth=self.line_width,\n",
        "                alpha=self.opacity\n",
        "            )\n",
        "            ax.add_patch(circle)\n",
        "\n",
        "            # Add ID number\n",
        "            ax.text(\n",
        "                x, y, str(dancer_id),\n",
        "                color=\"white\",\n",
        "                fontsize=12,\n",
        "                fontweight=\"bold\",\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                alpha=1.0\n",
        "            )\n",
        "\n",
        "        # Draw center stage marker (red X)\n",
        "        ax.plot(\n",
        "            self.stage_width / 2,\n",
        "            self.stage_height / 2,\n",
        "            \"x\",\n",
        "            color=\"#FF1744\",\n",
        "            markersize=10,\n",
        "            markeredgewidth=2\n",
        "        )\n",
        "\n",
        "        # Add orientation labels\n",
        "        ax.text(\n",
        "            self.stage_width / 2, -0.5,\n",
        "            \"AUDIENCE\",\n",
        "            ha=\"center\", va=\"top\",\n",
        "            color=\"white\", fontsize=10\n",
        "        )\n",
        "        ax.text(\n",
        "            self.stage_width / 2, self.stage_height + 0.5,\n",
        "            \"BACKSTAGE\",\n",
        "            ha=\"center\", va=\"bottom\",\n",
        "            color=\"white\", fontsize=10\n",
        "        )\n",
        "\n",
        "        # Configure plot appearance\n",
        "        ax.set_xlim(-0.5, self.stage_width + 0.5)\n",
        "        ax.set_ylim(-1.0, self.stage_height + 1.0)\n",
        "        ax.set_aspect(\"equal\")\n",
        "        ax.axis(\"off\")\n",
        "        fig.tight_layout(pad=0)\n",
        "\n",
        "        # Convert matplotlib figure to numpy array\n",
        "        canvas = FigureCanvasAgg(fig)\n",
        "        canvas.draw()\n",
        "        buffer = canvas.buffer_rgba()\n",
        "        image = np.asarray(buffer)[:, :, :3].copy()  # Drop alpha channel\n",
        "        plt.close(fig)\n",
        "\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "umMvwVLYv9fx",
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dancer Position Predictor Class\n",
        "# Dancer Position Predictor Class\n",
        "\n",
        "\"\"\"\n",
        "Main tracking class that processes video frames to extract dancer positions.\n",
        "This entire class must be in ONE CELL to maintain proper indentation.\n",
        "\"\"\"\n",
        "\n",
        "class DancerPositionEstimator:\n",
        "    \"\"\"\n",
        "    Main class for tracking dancer positions across video frames.\n",
        "\n",
        "    This class implements a robust multi-stage detection pipeline:\n",
        "    1. Primary detection using pose estimation (YOLOv8-pose)\n",
        "    2. Fallback to object detection if insufficient dancers found\n",
        "    3. Formation-aware tracking across frames\n",
        "    4. Velocity and rigid transformation prediction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_dancers,\n",
        "        model_size=\"m\",\n",
        "        stage_width=16,\n",
        "        stage_height=10,\n",
        "        size_weight=0.2,\n",
        "        formation_inertia=0.6,\n",
        "        rigid_threshold=1.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dancer position estimator.\n",
        "\n",
        "        Args:\n",
        "            num_dancers (int): Expected number of dancers to track\n",
        "            model_size (str): YOLO model size - 'n' (nano), 'm' (medium), 'l' (large)\n",
        "            stage_width (float): Virtual stage width in grid units\n",
        "            stage_height (float): Virtual stage depth in grid units\n",
        "            size_weight (float): Weight for size-based depth estimation (0-1)\n",
        "            formation_inertia (float): How much to trust formation rigidity (0-1)\n",
        "            rigid_threshold (float): Maximum distance for rigid transformation inliers\n",
        "        \"\"\"\n",
        "        assert num_dancers > 0, \"Number of dancers must be positive\"\n",
        "\n",
        "        self.num_dancers = int(num_dancers)\n",
        "        self.stage_width = float(stage_width)\n",
        "        self.stage_height = float(stage_height)\n",
        "        self.size_weight = float(size_weight)\n",
        "        self.formation_inertia = float(formation_inertia)\n",
        "        self.rigid_threshold = float(rigid_threshold)\n",
        "\n",
        "        # Load YOLO models\n",
        "        print(f\"Loading YOLOv8{model_size} models...\")\n",
        "        self.pose_model = YOLO(f\"yolov8{model_size}-pose.pt\")\n",
        "        self.detector_model = YOLO(f\"yolov8{model_size}.pt\")\n",
        "        print(\"✅ Models loaded successfully!\")\n",
        "\n",
        "        # Keypoint indices for ankles (COCO format)\n",
        "        self.left_ankle_idx = 15\n",
        "        self.right_ankle_idx = 16\n",
        "\n",
        "        # Tracking state\n",
        "        self.last_positions = None      # Current frame positions\n",
        "        self.previous_positions = None  # Previous frame positions (for velocity)\n",
        "        self.initialized = False\n",
        "\n",
        "    def _get_pose_candidates(self, image, confidence=0.25, use_flip=False):\n",
        "        \"\"\"\n",
        "        Extract pose-based dancer candidates from an image.\n",
        "        \"\"\"\n",
        "        height, width = image.shape[:2]\n",
        "        candidates = []\n",
        "\n",
        "        def process_image(img, is_flipped=False):\n",
        "            \"\"\"Process single image through pose model.\"\"\"\n",
        "            results = self.pose_model(img, conf=confidence, verbose=False)\n",
        "            if not results or results[0].boxes is None:\n",
        "                return\n",
        "\n",
        "            result = results[0]\n",
        "            boxes = result.boxes\n",
        "            keypoints = getattr(result, \"keypoints\", None)\n",
        "\n",
        "            # Sort by confidence\n",
        "            order = sorted(\n",
        "                range(len(boxes)),\n",
        "                key=lambda i: _float(boxes.conf[i]),\n",
        "                reverse=True\n",
        "            )\n",
        "\n",
        "            for i in order:\n",
        "                try:\n",
        "                    box = boxes[i]\n",
        "                    conf_score = _float(box.conf)\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(np.float32)\n",
        "\n",
        "                    # Adjust coordinates if image was flipped\n",
        "                    if is_flipped:\n",
        "                        x1, x2 = width - x2, width - x1\n",
        "\n",
        "                    # Try to get ankle position from keypoints\n",
        "                    ankle_point = None\n",
        "                    if keypoints and keypoints.xy is not None:\n",
        "                        kp_coords = keypoints.xy[i].cpu().numpy().astype(np.float32)\n",
        "                        ankle_coords = []\n",
        "\n",
        "                        # Check both ankles\n",
        "                        for ankle_idx in (self.left_ankle_idx, self.right_ankle_idx):\n",
        "                            if ankle_idx < kp_coords.shape[0]:\n",
        "                                ax, ay = kp_coords[ankle_idx]\n",
        "                                if is_flipped:\n",
        "                                    ax = width - ax\n",
        "                                # Validate coordinates\n",
        "                                if np.isfinite(ax) and np.isfinite(ay) and ax > 0 and ay > 0:\n",
        "                                    ankle_coords.append((ax, ay))\n",
        "\n",
        "                        # Use average of detected ankles\n",
        "                        if ankle_coords:\n",
        "                            ankle_x = float(np.mean([a[0] for a in ankle_coords]))\n",
        "                            ankle_y = float(np.mean([a[1] for a in ankle_coords]))\n",
        "                            ankle_point = (ankle_x, ankle_y)\n",
        "\n",
        "                    # Fallback: use bottom-center of bounding box\n",
        "                    if ankle_point is None:\n",
        "                        ankle_point = ((x1 + x2) * 0.5, y2)\n",
        "\n",
        "                    candidates.append({\n",
        "                        \"bbox\": [x1, y1, x2, y2],\n",
        "                        \"ankle\": ankle_point,\n",
        "                        \"conf\": conf_score\n",
        "                    })\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        # Process original image\n",
        "        process_image(image, is_flipped=False)\n",
        "\n",
        "        # Process flipped image for better edge detection\n",
        "        if use_flip:\n",
        "            flipped = np.ascontiguousarray(image[:, ::-1, :])\n",
        "            process_image(flipped, is_flipped=True)\n",
        "\n",
        "        return candidates\n",
        "\n",
        "    def _get_all_candidates(self, image, confidence):\n",
        "        \"\"\"\n",
        "        Get candidates from both original and enhanced images.\n",
        "        \"\"\"\n",
        "        # Detect on original image\n",
        "        candidates_raw = self._get_pose_candidates(image, confidence, use_flip=True)\n",
        "\n",
        "        # Detect on enhanced image (better for dark areas)\n",
        "        enhanced = enhance_frame(image)\n",
        "        candidates_enhanced = self._get_pose_candidates(enhanced, confidence, use_flip=True)\n",
        "\n",
        "        # Merge and remove duplicates\n",
        "        return nms_merge(candidates_raw + candidates_enhanced, iou_threshold=0.5)\n",
        "\n",
        "    def _get_detector_candidates(self, image, confidence=0.2):\n",
        "        \"\"\"\n",
        "        Fallback detection using general object detector.\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # Try both original and enhanced images\n",
        "        for img in (image, enhance_frame(image)):\n",
        "            try:\n",
        "                # Detect class 0 (person) only\n",
        "                results = self.detector_model(\n",
        "                    img, conf=confidence, classes=[0], verbose=False\n",
        "                )\n",
        "                if results and results[0].boxes is not None:\n",
        "                    result = results[0]\n",
        "                    boxes = result.boxes\n",
        "\n",
        "                    order = sorted(\n",
        "                        range(len(boxes)),\n",
        "                        key=lambda i: _float(boxes.conf[i]),\n",
        "                        reverse=True\n",
        "                    )\n",
        "\n",
        "                    for i in order:\n",
        "                        box = boxes[i]\n",
        "                        conf_score = _float(box.conf)\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(np.float32)\n",
        "\n",
        "                        # Use bottom-center as ankle approximation\n",
        "                        ankle_point = ((x1 + x2) * 0.5, y2)\n",
        "\n",
        "                        candidates.append({\n",
        "                            \"bbox\": [x1, y1, x2, y2],\n",
        "                            \"ankle\": ankle_point,\n",
        "                            \"conf\": conf_score\n",
        "                        })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return nms_merge(candidates, iou_threshold=0.5)\n",
        "\n",
        "    def _map_to_stage(self, ankle_xy, bbox_height, img_width, img_height, median_height):\n",
        "        \"\"\"\n",
        "        Map image pixel coordinates to stage grid coordinates.\n",
        "        \"\"\"\n",
        "        ax, ay = ankle_xy\n",
        "\n",
        "        # Normalize x-coordinate to [0, 1]\n",
        "        x_normalized = np.clip(ax / img_width, 0, 1)\n",
        "\n",
        "        # Normalize y-coordinate to [0, 1]\n",
        "        y_normalized = np.clip(ay / img_height, 0, 1)\n",
        "\n",
        "        # Invert y-axis (top of image = back of stage)\n",
        "        y_from_position = 1.0 - y_normalized\n",
        "\n",
        "        # Estimate depth from relative size\n",
        "        if median_height > 0:\n",
        "            size_ratio = median_height / max(1.0, bbox_height)\n",
        "            # Smaller dancers are assumed to be further away\n",
        "            size_depth = np.clip(size_ratio / 1.5, 0.0, 1.0)\n",
        "        else:\n",
        "            size_depth = y_from_position\n",
        "\n",
        "        # Combine position and size cues\n",
        "        y_stage = (1.0 - self.size_weight) * y_from_position + self.size_weight * size_depth\n",
        "        y_stage = float(np.clip(y_stage, 0, 1))\n",
        "\n",
        "        # Map to stage coordinates\n",
        "        return np.array([\n",
        "            x_normalized * self.stage_width,\n",
        "            y_stage * self.stage_height\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def _compute_cost_matrix(self, predicted, measured):\n",
        "        \"\"\"\n",
        "        Compute assignment cost matrix using spatial distance.\n",
        "        \"\"\"\n",
        "        num_pred = predicted.shape[0]\n",
        "        num_meas = measured.shape[0]\n",
        "        cost = np.zeros((num_pred, num_meas), dtype=np.float32)\n",
        "\n",
        "        for i in range(num_pred):\n",
        "            for j in range(num_meas):\n",
        "                dx = predicted[i, 0] - measured[j, 0]\n",
        "                dy = predicted[i, 1] - measured[j, 1]\n",
        "                # Euclidean distance plus small depth preference\n",
        "                cost[i, j] = np.hypot(dx, dy) + 0.1 * abs(dy)\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def _predict_with_velocity(self):\n",
        "        \"\"\"\n",
        "        Predict next positions using constant velocity model.\n",
        "        \"\"\"\n",
        "        if self.last_positions is None:\n",
        "            return None\n",
        "        if self.previous_positions is None:\n",
        "            return self.last_positions.copy()\n",
        "\n",
        "        # Compute velocity from last two frames\n",
        "        velocity = self.last_positions - self.previous_positions\n",
        "        # Extrapolate\n",
        "        return self.last_positions + velocity\n",
        "\n",
        "    def _initialize_positions(self, measurements):\n",
        "        \"\"\"\n",
        "        Initialize dancer positions for the first frame.\n",
        "        \"\"\"\n",
        "        if measurements.shape[0] == 0:\n",
        "            # No detections: place dancers in a line across stage\n",
        "            y = self.stage_height * 0.5\n",
        "            xs = np.linspace(\n",
        "                self.stage_width * 0.2,\n",
        "                self.stage_width * 0.8,\n",
        "                self.num_dancers,\n",
        "                dtype=np.float32\n",
        "            )\n",
        "            positions = np.stack(\n",
        "                [xs, np.full_like(xs, y, dtype=np.float32)],\n",
        "                axis=1\n",
        "            )\n",
        "            self.previous_positions = None\n",
        "            self.last_positions = positions.copy()\n",
        "            self.initialized = True\n",
        "            return\n",
        "\n",
        "        # Sort detections by x-coordinate\n",
        "        measurements = measurements[np.argsort(measurements[:, 0])]\n",
        "        count = measurements.shape[0]\n",
        "        median_y = float(np.median(measurements[:, 1]))\n",
        "        min_x = float(np.min(measurements[:, 0]))\n",
        "        max_x = float(np.max(measurements[:, 0]))\n",
        "\n",
        "        # Ensure reasonable spread\n",
        "        if max_x - min_x < 1e-3:\n",
        "            min_x = self.stage_width * 0.2\n",
        "            max_x = self.stage_width * 0.8\n",
        "\n",
        "        # Create evenly spaced slots\n",
        "        slots_x = np.linspace(min_x, max_x, self.num_dancers, dtype=np.float32)\n",
        "        assigned = [False] * count\n",
        "        positions = np.zeros((self.num_dancers, 2), dtype=np.float32)\n",
        "\n",
        "        # Assign detections to nearest slots\n",
        "        for k in range(self.num_dancers):\n",
        "            slot_x = float(slots_x[k])\n",
        "            best_idx = -1\n",
        "            best_dist = 1e9\n",
        "\n",
        "            for j in range(count):\n",
        "                if assigned[j]:\n",
        "                    continue\n",
        "                dist = abs(float(measurements[j, 0]) - slot_x)\n",
        "                if dist < best_dist:\n",
        "                    best_dist = dist\n",
        "                    best_idx = j\n",
        "\n",
        "            if best_idx >= 0:\n",
        "                positions[k] = measurements[best_idx]\n",
        "                assigned[best_idx] = True\n",
        "            else:\n",
        "                # Fill gap with interpolated position\n",
        "                positions[k] = np.array([slot_x, median_y], dtype=np.float32)\n",
        "\n",
        "        self.previous_positions = None\n",
        "        self.last_positions = positions\n",
        "        self.initialized = True\n",
        "\n",
        "    def process_frame(self, image):\n",
        "        \"\"\"\n",
        "        Process a single video frame to extract all dancer positions.\n",
        "\n",
        "        This is the main entry point that orchestrates the entire detection\n",
        "        and tracking pipeline for each frame.\n",
        "        \"\"\"\n",
        "        height, width = image.shape[:2]\n",
        "\n",
        "        # STAGE 1: Multi-cascade detection\n",
        "        try:\n",
        "            # Try standard confidence threshold\n",
        "            candidates = self._get_all_candidates(image, confidence=0.25)\n",
        "\n",
        "            # If not enough dancers found, try lower threshold\n",
        "            if len(candidates) < self.num_dancers:\n",
        "                more = self._get_all_candidates(image, confidence=0.10)\n",
        "                candidates = nms_merge(candidates + more, iou_threshold=0.5)\n",
        "\n",
        "            # Last resort: use object detector\n",
        "            if len(candidates) < self.num_dancers:\n",
        "                detector_cands = self._get_detector_candidates(image, confidence=0.20)\n",
        "                candidates = nms_merge(candidates + detector_cands, iou_threshold=0.5)\n",
        "        except Exception:\n",
        "            candidates = []\n",
        "\n",
        "        # STAGE 2: Map to stage coordinates\n",
        "        if len(candidates) > 0:\n",
        "            # Calculate median height for depth estimation\n",
        "            heights = [max(1.0, c[\"bbox\"][3] - c[\"bbox\"][1]) for c in candidates]\n",
        "            median_height = float(np.median(heights))\n",
        "\n",
        "            stage_measurements = []\n",
        "            for c in candidates:\n",
        "                bbox = c[\"bbox\"]\n",
        "                h = max(1.0, bbox[3] - bbox[1])\n",
        "                stage_pos = self._map_to_stage(\n",
        "                    c[\"ankle\"], h, width, height, median_height\n",
        "                )\n",
        "                stage_measurements.append(stage_pos)\n",
        "\n",
        "            stage_measurements = np.array(stage_measurements, dtype=np.float32)\n",
        "        else:\n",
        "            stage_measurements = np.zeros((0, 2), dtype=np.float32)\n",
        "\n",
        "        # STAGE 3: Handle initialization\n",
        "        if not self.initialized:\n",
        "            # First frame: initialize positions\n",
        "            if stage_measurements.shape[0] > self.num_dancers:\n",
        "                # Too many detections: keep highest confidence ones\n",
        "                order = np.argsort([-c[\"conf\"] for c in candidates])[:self.num_dancers]\n",
        "                stage_measurements = np.array(\n",
        "                    [stage_measurements[i] for i in order],\n",
        "                    dtype=np.float32\n",
        "                )\n",
        "            self._initialize_positions(stage_measurements)\n",
        "\n",
        "        # Validate state\n",
        "        if self.last_positions is None or self.last_positions.shape != (self.num_dancers, 2):\n",
        "            self._initialize_positions(stage_measurements)\n",
        "\n",
        "        # STAGE 4: Predict next positions\n",
        "        # Method 1: Velocity-based prediction\n",
        "        predicted_velocity = self._predict_with_velocity()\n",
        "        if predicted_velocity is None:\n",
        "            predicted_velocity = self.last_positions.copy()\n",
        "\n",
        "        # Method 2: Formation-based prediction (rigid transformation)\n",
        "        predicted_rigid = None\n",
        "        if stage_measurements.shape[0] >= 2:\n",
        "            # Find correspondences for rigid transformation\n",
        "            cost_matrix = self._compute_cost_matrix(\n",
        "                self.last_positions, stage_measurements\n",
        "            )\n",
        "            rows, cols = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            # Find inliers (good matches)\n",
        "            inlier_rows, inlier_cols = [], []\n",
        "            for r, c in zip(rows, cols):\n",
        "                if (r < self.num_dancers and\n",
        "                    c < stage_measurements.shape[0] and\n",
        "                    cost_matrix[r, c] <= self.rigid_threshold):\n",
        "                    inlier_rows.append(r)\n",
        "                    inlier_cols.append(c)\n",
        "\n",
        "            # Estimate rigid transformation if enough inliers\n",
        "            if len(inlier_rows) >= 2:\n",
        "                src = self.last_positions[inlier_rows]\n",
        "                dst = stage_measurements[inlier_cols]\n",
        "                try:\n",
        "                    scale, rotation, translation = umeyama(src, dst, estimate_scale=True)\n",
        "                    # Apply transformation to all positions\n",
        "                    predicted_rigid = (scale * (self.last_positions @ rotation.T)) + translation\n",
        "                except Exception:\n",
        "                    predicted_rigid = None\n",
        "\n",
        "        # Combine predictions\n",
        "        if predicted_rigid is not None:\n",
        "            # Weighted combination of velocity and formation predictions\n",
        "            predicted = (\n",
        "                self.formation_inertia * predicted_rigid +\n",
        "                (1.0 - self.formation_inertia) * predicted_velocity\n",
        "            )\n",
        "        else:\n",
        "            predicted = predicted_velocity\n",
        "\n",
        "        # STAGE 5: Assign measurements to tracks\n",
        "        if stage_measurements.shape[0] > 0:\n",
        "            # Solve assignment problem\n",
        "            cost_matrix = self._compute_cost_matrix(predicted, stage_measurements)\n",
        "            rows, cols = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "            new_positions = predicted.copy()\n",
        "            used_measurements = set()\n",
        "            deltas = []\n",
        "\n",
        "            # Apply assigned measurements\n",
        "            for r, c in zip(rows, cols):\n",
        "                if r < self.num_dancers and c < stage_measurements.shape[0]:\n",
        "                    new_positions[r] = stage_measurements[c]\n",
        "                    used_measurements.add(c)\n",
        "                    deltas.append(stage_measurements[c] - predicted[r])\n",
        "\n",
        "            # For unassigned tracks, apply median motion\n",
        "            if len(deltas) > 0:\n",
        "                median_delta = np.median(np.stack(deltas, axis=0), axis=0)\n",
        "            else:\n",
        "                median_delta = np.zeros(2, dtype=np.float32)\n",
        "\n",
        "            for r in range(self.num_dancers):\n",
        "                if not any(row == r for row, _ in zip(rows, cols)):\n",
        "                    # No measurement assigned: use prediction + median motion\n",
        "                    new_positions[r] = predicted[r] + median_delta\n",
        "        else:\n",
        "            # No measurements: use pure prediction\n",
        "            new_positions = predicted.copy()\n",
        "\n",
        "        # STAGE 6: Constrain to stage boundaries\n",
        "        new_positions[:, 0] = np.clip(new_positions[:, 0], 0.0, self.stage_width)\n",
        "        new_positions[:, 1] = np.clip(new_positions[:, 1], 0.0, self.stage_height)\n",
        "\n",
        "        # Update state for next frame\n",
        "        self.previous_positions = self.last_positions\n",
        "        self.last_positions = new_positions\n",
        "\n",
        "        # Format output\n",
        "        return [\n",
        "            {\n",
        "                \"id\": k + 1,\n",
        "                \"x\": float(self.last_positions[k, 0]),\n",
        "                \"y\": float(self.last_positions[k, 1])\n",
        "            }\n",
        "            for k in range(self.num_dancers)\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "Kgbx78kkwI1D",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Video Processing Function\n",
        "# Video Processing Function\n",
        "\n",
        "\"\"\"\n",
        "Functions for sampling frames and processing complete videos.\n",
        "\"\"\"\n",
        "\n",
        "def sample_video_frames(video_path, num_frames=10):\n",
        "    \"\"\"\n",
        "    Sample frames evenly from a video file.\n",
        "\n",
        "    Extracts frames at regular intervals throughout the video duration\n",
        "    to get a representative sample of the entire performance.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to input video file\n",
        "        num_frames (int): Number of frames to extract\n",
        "\n",
        "    Returns:\n",
        "        list: List of RGB image arrays\n",
        "    \"\"\"\n",
        "    clip = VideoFileClip(video_path)\n",
        "    duration = clip.duration\n",
        "\n",
        "    # Calculate timestamps for even sampling\n",
        "    timestamps = np.linspace(0.0, max(0.0001, duration - 1e-6), num_frames)\n",
        "\n",
        "    frames = []\n",
        "    for i, t in enumerate(timestamps):\n",
        "        print(f\"Extracting frame {i+1}/{num_frames} at {t:.2f}s...\")\n",
        "        frame = np.array(clip.get_frame(float(t)), dtype=np.uint8, copy=True, order=\"C\")\n",
        "        frames.append(frame)\n",
        "\n",
        "    clip.close()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def process_dance_video(\n",
        "    video_path,\n",
        "    expected_dancers,\n",
        "    model_size=\"m\",\n",
        "    num_frames=10\n",
        "):\n",
        "    \"\"\"\n",
        "    Main pipeline to process a dance video and generate formation visualizations.\n",
        "\n",
        "    This function orchestrates the entire workflow:\n",
        "    1. Validates inputs\n",
        "    2. Samples frames from video\n",
        "    3. Processes each frame to extract positions\n",
        "    4. Generates formation visualizations\n",
        "    5. Creates composite images\n",
        "    6. Packages results in a ZIP file\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to input video file\n",
        "        expected_dancers (int): Number of dancers to track\n",
        "        model_size (str): YOLO model size ('n', 'm', or 'l')\n",
        "        num_frames (int): Number of frames to sample\n",
        "\n",
        "    Returns:\n",
        "        tuple: (composite_paths, zip_path, status_message)\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    if video_path is None or not os.path.exists(video_path):\n",
        "        return None, None, \"⚠️ Please upload a video file.\"\n",
        "\n",
        "    try:\n",
        "        num_dancers = int(float(expected_dancers))\n",
        "    except Exception:\n",
        "        return None, None, \"⚠️ Number of dancers must be a valid number.\"\n",
        "\n",
        "    if num_dancers <= 0:\n",
        "        return None, None, \"⚠️ Number of dancers must be positive.\"\n",
        "\n",
        "    if num_dancers > 20:\n",
        "        return None, None, \"⚠️ Maximum 20 dancers supported.\"\n",
        "\n",
        "    print(f\"\\n🎬 Processing video: {video_path}\")\n",
        "    print(f\"👥 Tracking {num_dancers} dancers\")\n",
        "    print(f\"🖼️ Sampling {num_frames} frames\")\n",
        "\n",
        "    # Sample frames from video\n",
        "    frames = sample_video_frames(video_path, num_frames)\n",
        "\n",
        "    # Setup temporary directory for outputs\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    composites_dir = _ensure_dir(os.path.join(temp_dir, \"composites\"))\n",
        "\n",
        "    # Initialize components with fixed parameters\n",
        "    estimator = DancerPositionEstimator(\n",
        "        num_dancers=num_dancers,\n",
        "        model_size=model_size,\n",
        "        stage_width=16,\n",
        "        stage_height=10,\n",
        "        size_weight=0.2,\n",
        "        formation_inertia=0.6,\n",
        "        rigid_threshold=1.0\n",
        "    )\n",
        "\n",
        "    renderer = FormationRenderer(stage_width=16, stage_height=10)\n",
        "\n",
        "    composite_paths = []\n",
        "\n",
        "    # Process each frame\n",
        "    print(\"\\n🔄 Processing frames...\")\n",
        "    for i, frame in enumerate(frames):\n",
        "        print(f\"Processing frame {i+1}/{num_frames}...\")\n",
        "        try:\n",
        "            height, width = frame.shape[:2]\n",
        "\n",
        "            # Extract dancer positions\n",
        "            positions = estimator.process_frame(frame)\n",
        "\n",
        "            # Render formation visualization\n",
        "            formation_image = renderer.render(\n",
        "                positions,\n",
        "                target_width=width,\n",
        "                target_height=height\n",
        "            )\n",
        "\n",
        "            # Create composite image (original on top, formation on bottom)\n",
        "            top_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            bottom_bgr = cv2.cvtColor(formation_image, cv2.COLOR_RGB2BGR)\n",
        "            composite = np.vstack([top_bgr, bottom_bgr])\n",
        "\n",
        "            # Save high-quality JPEG\n",
        "            output_file = os.path.join(composites_dir, f\"composite_{i:02d}.jpg\")\n",
        "            cv2.imwrite(output_file, composite, [cv2.IMWRITE_JPEG_QUALITY, 95])\n",
        "            composite_paths.append(output_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error processing frame {i}: {e}\")\n",
        "            # Create error placeholder\n",
        "            error_image = np.zeros((frame.shape[0] * 2, frame.shape[1], 3), dtype=np.uint8)\n",
        "            cv2.putText(\n",
        "                error_image,\n",
        "                f\"Frame {i} processing error\",\n",
        "                (10, 40),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1,\n",
        "                (255, 255, 255),\n",
        "                2\n",
        "            )\n",
        "            output_file = os.path.join(composites_dir, f\"composite_{i:02d}.jpg\")\n",
        "            cv2.imwrite(output_file, error_image)\n",
        "            composite_paths.append(output_file)\n",
        "\n",
        "    # Create ZIP archive\n",
        "    print(\"\\n📦 Creating ZIP archive...\")\n",
        "    zip_path = os.path.join(temp_dir, \"dance_formations.zip\")\n",
        "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
        "        for path in composite_paths:\n",
        "            zip_file.write(path, arcname=os.path.basename(path))\n",
        "\n",
        "    print(\"✅ Processing complete!\")\n",
        "    return composite_paths, zip_path, \"✅ Processing complete! Download your results below.\"\n"
      ],
      "metadata": {
        "id": "fRrKoqR3wcuA",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GUI Interface\n",
        "# GUI - Gradio Interface\n",
        "\n",
        "\"\"\"\n",
        "Creates the web-based user interface with purple theme.\n",
        "\"\"\"\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"\n",
        "    Create the Gradio web interface for the Dance Formation Tracker.\n",
        "\n",
        "    This function builds a user-friendly web interface with:\n",
        "    - Video upload capability\n",
        "    - Parameter controls (dancers, model, frames)\n",
        "    - Results gallery\n",
        "    - Download functionality\n",
        "    - Purple-themed styling\n",
        "    \"\"\"\n",
        "\n",
        "    # Custom CSS for purple theme\n",
        "    custom_css = \"\"\"\n",
        "    /* Main container styling */\n",
        "    .gradio-container {\n",
        "        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n",
        "    }\n",
        "\n",
        "    /* Purple gradient buttons */\n",
        "    .gr-button-primary {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        border: none;\n",
        "        color: white;\n",
        "        font-weight: 600;\n",
        "        padding: 12px 24px;\n",
        "        font-size: 16px;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "\n",
        "    .gr-button-primary:hover {\n",
        "        background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);\n",
        "        transform: translateY(-2px);\n",
        "        box-shadow: 0 10px 20px rgba(102, 126, 234, 0.4);\n",
        "    }\n",
        "\n",
        "    /* Input field styling */\n",
        "    .gr-input {\n",
        "        border-color: #667eea;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "\n",
        "    .gr-input:focus {\n",
        "        border-color: #764ba2;\n",
        "        box-shadow: 0 0 0 3px rgba(118, 75, 162, 0.1);\n",
        "    }\n",
        "\n",
        "    /* Heading styles */\n",
        "    h1 {\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        -webkit-background-clip: text;\n",
        "        -webkit-text-fill-color: transparent;\n",
        "        font-size: 2.5em;\n",
        "        font-weight: 700;\n",
        "        margin-bottom: 0.5em;\n",
        "    }\n",
        "\n",
        "    h2 {\n",
        "        color: #764ba2;\n",
        "        font-weight: 600;\n",
        "    }\n",
        "\n",
        "    h3 {\n",
        "        color: #667eea;\n",
        "        font-weight: 600;\n",
        "    }\n",
        "\n",
        "    /* Gallery styling */\n",
        "    .gallery-container {\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    /* Status box styling */\n",
        "    .gr-textbox {\n",
        "        border-left: 4px solid #667eea;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    # Create interface with purple theme\n",
        "    with gr.Blocks(\n",
        "        title=\"Dance Formation Tracker\",\n",
        "        theme=gr.themes.Soft(\n",
        "            primary_hue=\"purple\",\n",
        "            secondary_hue=\"pink\",\n",
        "            neutral_hue=\"slate\",\n",
        "        ),\n",
        "        css=custom_css\n",
        "    ) as interface:\n",
        "\n",
        "        # Header section\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # 🩰 Dance Formation Tracker\n",
        "\n",
        "            **Transform dance videos into clear formation diagrams using AI-powered tracking**\n",
        "\n",
        "            This tool uses advanced computer vision to track dancers throughout a performance\n",
        "            and generate bird's-eye view formation diagrams showing their positions on stage.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Main content in two columns\n",
        "        with gr.Row():\n",
        "            # Left column: Input controls\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 📤 Video Settings\")\n",
        "\n",
        "                video_input = gr.File(\n",
        "                    label=\"Upload Dance Video\",\n",
        "                    file_types=[\"video\"],\n",
        "                    type=\"filepath\"\n",
        "                )\n",
        "\n",
        "                dancers_input = gr.Number(\n",
        "                    label=\"Number of Dancers\",\n",
        "                    value=8,\n",
        "                    minimum=1,\n",
        "                    maximum=20,\n",
        "                    step=1,\n",
        "                    info=\"How many dancers are performing in the video?\"\n",
        "                )\n",
        "\n",
        "                frames_slider = gr.Slider(\n",
        "                    minimum=5,\n",
        "                    maximum=30,\n",
        "                    value=10,\n",
        "                    step=1,\n",
        "                    label=\"Frame Sampling Interval\",\n",
        "                    info=\"Number of frames to analyze (more = detailed but slower)\"\n",
        "                )\n",
        "\n",
        "                model_selector = gr.Radio(\n",
        "                    choices=[\n",
        "                        (\"Fast (nano)\", \"n\"),\n",
        "                        (\"Balanced (medium)\", \"m\"),\n",
        "                        (\"Accurate (large)\", \"l\")\n",
        "                    ],\n",
        "                    value=\"m\",\n",
        "                    label=\"Model Quality\",\n",
        "                    info=\"Trade-off between speed and accuracy\"\n",
        "                )\n",
        "\n",
        "                process_button = gr.Button(\n",
        "                    \"🎭 Generate Formation Analysis\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "                # Tips section\n",
        "                gr.Markdown(\n",
        "                    \"\"\"\n",
        "                    ---\n",
        "                    **📝 Tips for Best Results:**\n",
        "                    - Use videos with all dancers visible\n",
        "                    - Ensure adequate lighting\n",
        "                    - Full body shots work better\n",
        "                    - Steady camera position preferred\n",
        "                    - Higher frame counts = smoother tracking\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "            # Right column: Results display\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### 📊 Analysis Results\")\n",
        "\n",
        "                gallery_output = gr.Gallery(\n",
        "                    label=\"Formation Analysis (Top: Original | Bottom: Stage Formation)\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    height=\"auto\",\n",
        "                    show_label=True,\n",
        "                    elem_classes=\"gallery-container\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    download_output = gr.File(\n",
        "                        label=\"📥 Download All Results (ZIP)\",\n",
        "                        visible=True\n",
        "                    )\n",
        "\n",
        "                    status_output = gr.Textbox(\n",
        "                        label=\"Status\",\n",
        "                        lines=1,\n",
        "                        interactive=False,\n",
        "                        value=\"Ready to process your video...\"\n",
        "                    )\n",
        "\n",
        "        # Connect processing function\n",
        "        def run_analysis(video, dancers, frames, model):\n",
        "            \"\"\"Wrapper function to run the analysis.\"\"\"\n",
        "            return process_dance_video(\n",
        "                video_path=video,\n",
        "                expected_dancers=dancers,\n",
        "                model_size=model,\n",
        "                num_frames=int(frames)\n",
        "            )\n",
        "\n",
        "        process_button.click(\n",
        "            fn=run_analysis,\n",
        "            inputs=[video_input, dancers_input, frames_slider, model_selector],\n",
        "            outputs=[gallery_output, download_output, status_output]\n",
        "        )\n",
        "\n",
        "        # Information section\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ---\n",
        "            ### 💡 How It Works\n",
        "\n",
        "            1. **Upload** your dance video file\n",
        "            2. **Specify** the number of dancers performing\n",
        "            3. **Choose** analysis settings (frame interval and model quality)\n",
        "            4. **Click** Generate to start processing\n",
        "            5. **Review** the formation diagrams for each sampled frame\n",
        "            6. **Download** all results as a ZIP file\n",
        "\n",
        "            The system uses state-of-the-art pose estimation to:\n",
        "            - Track individual dancers throughout the video\n",
        "            - Map their positions to a virtual stage grid\n",
        "            - Maintain consistent ID assignment across frames\n",
        "            - Handle challenging conditions like occlusion and poor lighting\n",
        "\n",
        "            ---\n",
        "            *Built with YOLOv8, OpenCV, and Gradio*\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    return interface\n"
      ],
      "metadata": {
        "id": "LNWHA-8wwmr9",
        "cellView": "form"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute Code\n",
        "# Run\n",
        "\n",
        "\"\"\"\n",
        "Main entry point to create and launch the web interface.\n",
        "Run this cell to start the application.\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Starting Dance Formation Tracker...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create the interface\n",
        "    app = create_gradio_interface()\n",
        "\n",
        "    # Launch with inline display for Colab\n",
        "    app.launch(\n",
        "        share=True,           # Still create public URL as backup\n",
        "        inline=True,          # Display interface inline in notebook\n",
        "        inbrowser=False,      # Don't try to open a new browser tab\n",
        "        show_error=True,      # Show detailed error messages\n",
        "        quiet=False,          # Show launch information\n",
        "        height=800,           # Set height of inline frame (adjust as needed)\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"✅ Application is running!\")\n",
        "    print(\"📱 Access the app using the public URL above\")\n",
        "    print(\"🔗 Look for the public URL that starts with https://...gradio.live\")\n",
        "    print(\"⚠️ If you need to restart, first stop the cell (interrupt execution) then run again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EXzkFgyruqOH",
        "outputId": "8dc23308-07af-4708-c586-08c69ed13df6",
        "cellView": "form"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Dance Formation Tracker...\n",
            "==================================================\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ae89941bbfeff47a41.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ae89941bbfeff47a41.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "✅ Application is running!\n",
            "📱 Access the app using the public URL above\n",
            "🔗 Look for the public URL that starts with https://...gradio.live\n",
            "⚠️ If you need to restart, first stop the cell (interrupt execution) then run again\n"
          ]
        }
      ]
    }
  ]
}